# Tunix: A JAX-native LLM Post-Training Library

<div align="left">

<a href="https://tunix.readthedocs.io/en/latest/index.html"><img src="https://img.shields.io/badge/documentation-blue"></a>

</div>

**Tunix(Tune-in-JAX)** is a JAX based library designed to streamline the
post-training of Large Language Models. It provides efficient and scalable
supports for:

- **Supervised Fine-Tuning**
- **Reinforcement Learning (RL)**
- **Knowledge Distillation**

Tunix leverages the power of JAX for accelerated computation and seamless
integration with JAX-based modeling framework
[Flax NNX](https://flax.readthedocs.io/en/latest/nnx_basics.html).

**Current Status: Early Development**

Tunix is in early development. We're actively working to expand its
capabilities, usability and improve its performance. Stay tuned for upcoming
updates and new features!

## Key Features & Highlights

Tunix is still under development, here's a glimpse of the current features:

- **Supervised Fine-Tuning:**
  - Full Weights Fine-Tuning
  - Parameter-Efficient Fine-Tuning (PEFT) with LoRA/Q-LoRA Layers
- **Reinforcement Learning (RL):**
  - Proximal Policy Optimization (PPO)
  - Group Relative Policy Optimization (GRPO)
  - Token-level Group Sequence Policy Optimization (GSPO-token)
- **Preference Fine-Tuning:**
  - Preference alignments with Direct Preference Optimization (DPO)
- **Knowledge Distillation:**
  - Logit Strategy: A classic approach where the student learns to match the
    teacher's output probability distribution.
  - Attention Transfer & Projection Strategies: Methods to align the attention
    mechanisms between the student and teacher models.
  - Feature Pooling & Projection Strategies: General techniques for matching
    intermediate feature representations, even between models of different
    architectures.
- **Modularity:**
  - Components are designed to be reusable and composable
  - Easy to customize and extend
- **Efficiency:**
  - Native support of common model sharding strategies such as DP, FSDP and TP
  - Designed for distributed training on accelerators (TPU)

## Upcoming

- **Agentic RL Training:**
  - Async Rollout
  - Multi-turn & multi-step support
  - Tool usage
- **Advanced Algorithms:**
  - Addtional state-of-the-art RL and distillation algorithms
- **Scalability:**
  - Multi-host distributed training
  - Optimized rollout with vLLM
- **User Guides:**
  - More advanced RL recipe

## Installation

You can install Tunix in several ways:

1. From PyPI (recommended):

```sh
pip install "google-tunix[prod]"
```

2. Directly from GitHub (latest main branch)

```sh
pip install git+https://github.com/google/tunix
```

3. From source (editable install) If you plan to modify the codebase and run it
   in development mode. If you'd like to install vllm, the tpu-inference
   supported version is not released yet, please follow the instructions to
   install manually
   (https://docs.vllm.ai/en/latest/getting_started/installation/google_tpu.html)
   or download the docker image (vllm/vllm-tpu:v0.11.1) then
   `pip install tpu-inference` for TPU backend:

```sh
git clone https://github.com/google/tunix.git
cd tunix
pip install -e ".[dev]"

# Then install vLLM and tpu-inference
```

## Getting Started

To get started, we have a bunch of detailed examples and tutorials.

- [PEFT Gemma with QLoRA](https://github.com/google/tunix/blob/main/examples/qlora_demo.ipynb)
- [Training Gemma on grade school Math problems using GRPO](https://github.com/google/tunix/blob/main/examples/grpo_demo.ipynb)
- [Logit Distillation using Gemma models](https://github.com/google/tunix/blob/main/examples/logit_distillation.ipynb)

To setup Jupyter notebook on single host GCP TPU VM, please refer to the
[setup script](https://github.com/google/tunix/blob/main/scripts/setup_notebook_tpu_single_host.sh).

We plan to provide clear, concise documentation and more examples in the near
future.

## Contributing and Feedbacks

We welcome contributions! As Tunix is in early development, the contribution
process is still being formalized. A rough draft of the contribution process is
present [here](https://github.com/google/tunix/blob/main/CONTRIBUTING.md). In
the meantime, you can make feature requests, report issues and ask questions in
our
[Tunix GitHub discussion forum](https://github.com/google/tunix/discussions).

## Collaborations and Partnership

[GRL](https://github.com/lmgame-org/GRL/blob/tunix_integration_dev/README.md)
(Game Reinforcement Learning), developed by
[Hao AI Lab](https://hao-ai-lab.github.io/) from UCSD, is an open-source
framework for post-training large language models through multi-turn RL on
challenging games. In collaboration with Tunix, GRL integrates seamless TPU
supportâ€”letting users quickly run scalable, reproducible RL experiments (like
PPO rollouts on Qwen2.5-0.5B-Instruct) on TPU v4 meshes with
[minimal setup](https://github.com/lmgame-org/GRL/blob/tunix_integration_dev/README.md#5-launch-the-quick-test-defaults-to-qwen2505b-supports-4-tpu-v4-with-mesh-22).
This partnership empowers the community to push LLM capabilities further,
combining Tunixâ€™s optimized TPU runtime with GRLâ€™s flexible game RL pipeline for
cutting-edge research and easy reproducibility.

## Stay Tuned!

Thank you for your interest in Tunix. We're working hard to bring you a powerful
and efficient library for LLM post-training. Please follow our progress and
check back for updates!

## Citing Tunix

```bibtex
@misc{tunix2025,
  title={Tunix},
  author={Bao, Tianshu and Wang, Lance and Sharma, Abheesht and Shin, Jiwon and
  Yan, Ann and Tan, Sizhi and Gao, Haoyu and Ha, Jen and Chai, Lin and
  Liu, Dangyi and Iyer, Rakesh and Sahu, Mridul and others},
  year={2025},
  howpublished={\url{https://github.com/google/tunix}},
}
```

## Acknowledgements

Thank you to all our wonderful contributors!

[![Contributors](https://contrib.rocks/image?repo=google/tunix)](https://github.com/google/tunix/graphs/contributors)

# CAL-GRPO: Fine-Grained Credit Assignment for RL Training

âœ… **Integration Tests Passed** - Ready for production experiments!

---

## ðŸŽ¯ Quick Start

### Run Both Experiments (Recommended)

```bash
source .venv/bin/activate

# Small test (10 minutes)
./run_experiments.sh 20 2

# Medium scale (2-3 hours)
./run_experiments.sh 100 4

# Publication quality (6-12 hours)
./run_experiments.sh 500 4
```

### Run Individual Experiments

```bash
# Baseline GRPO
python train_cal.py --num-samples 100 --num-generations 4

# CAL-GRPO (your research)
python train_cal.py --use-cal --num-samples 100 --num-generations 4
```

### Compare Results

```bash
python compare_results.py
```

---

## ðŸ“Š What You Get

After running experiments, you'll see:

```
========================================
BASELINE vs CAL-GRPO COMPARISON
========================================

Test Accuracy (%)     38.50      42.30      +3.80 (+9.9%)
Correct / Total       38/100     42/100             

âœ… CAL improves accuracy by 3.80 percentage points!
========================================
```

---

## ðŸŽ“ Experiment Scale Guide

| Goal | Samples | Time | Command |
|------|---------|------|---------|
| **Quick test** | 20 | 10 min | `./run_experiments.sh 20 2` |
| **Proof of concept** | 100 | 2 hrs | `./run_experiments.sh 100 4` |
| **Workshop paper** | 500 | 6 hrs | `./run_experiments.sh 500 4` |
| **Conference paper** | 7,473 | 24 hrs | `python train_cal.py --use-cal --num-samples -1` |

---

## ðŸ“ Key Files

- **`train_cal.py`** - Main training script (baseline & CAL)
- **`eval_math.py`** - Evaluation system
- **`compare_results.py`** - Results comparison
- **`run_experiments.sh`** - Automated experiment runner
- **`tunix/rl/cal/`** - CAL implementation

---

## ðŸ”§ Configuration

### Use GPT-4 Instead of GPT-3.5

```bash
python train_cal.py --use-cal --cal-model gpt-4
```

### Use Gemini API

Add to `.env`:
```bash
GEMINI_API_KEY=your_key
```

Then run:
```bash
python train_cal.py --use-cal --api-provider gemini --cal-model gemini-1.5-pro-latest
```

### Adjust CAL Parameters

```bash
python train_cal.py --use-cal \
  --negative-reward -2.0 \
  --max-error-span 128 \
  --cal-model gpt-4
```

---

## ðŸ“ˆ View Training Progress

```bash
tensorboard --logdir /tmp/cal_experiments/
# Open browser to http://localhost:6006
```

---

## ðŸ§ª Integration Test Results

âœ… **All systems verified** - See `INTEGRATION_TEST_RESULTS.md`

- Training: âœ… Working
- Evaluation: âœ… Working
- CAL Oracle: âœ… Working
- Comparison: âœ… Working

---

## ðŸ“š Documentation

- **Quick Start**: This file
- **Integration Tests**: `INTEGRATION_TEST_RESULTS.md`
- **Migration Plan**: `cal-to-tunix-migration.plan.md`
- **Demos**: `demos_and_tests/`

---

## ðŸŽ¯ Typical Workflow

1. **Test the setup**
   ```bash
   ./run_experiments.sh 20 2
   python compare_results.py
   ```

2. **Run proof-of-concept**
   ```bash
   ./run_experiments.sh 100 4
   ```

3. **Check if CAL helps**
   - If accuracy improves: Scale up!
   - If not: Tune hyperparameters

4. **Scale to publication quality**
   ```bash
   ./run_experiments.sh 500 4
   ```

5. **Analyze and write paper** ðŸ“

---

## ðŸ” Troubleshooting

### "OPENAI_API_KEY not found"
```bash
echo "OPENAI_API_KEY=sk-..." >> .env
```

### "TPU already in use"
```bash
pkill -9 python
sleep 5
```

### Check TPU availability
```bash
python -c "import jax; print(jax.devices())"
```

---

## âœ¨ What Was Migrated

Your FGC-PPO research from PyTorch/OAT to JAX/Tunix:

- âœ… CAL Oracle (OpenAI + Gemini support)
- âœ… Token-level error mapping
- âœ… Sparse reward construction
- âœ… Fine-grained advantage calculation
- âœ… Full evaluation system
- âœ… Comparison tools

**Everything works and is tested!**

---

## ðŸš€ Ready to Run!

Your CAL research is fully migrated and tested. Run your experiments and publish! ðŸ“„

# Tunix: A JAX-native LLM Post-Training Library

<div align="left">

<a href="https://tunix.readthedocs.io/en/latest/index.html"><img src="https://img.shields.io/badge/documentation-blue"></a>

</div>

**Tunix(Tune-in-JAX)** is a JAX based library designed to streamline the
post-training of Large Language Models. It provides efficient and scalable
supports for:

- **Supervised Fine-Tuning**
- **Reinforcement Learning (RL)**
- **Knowledge Distillation**

Tunix leverages the power of JAX for accelerated computation and seamless
integration with JAX-based modeling framework
[Flax NNX](https://flax.readthedocs.io/en/latest/nnx_basics.html).

**Current Status: Early Development**

Tunix is in early development. We're actively working to expand its
capabilities, usability and improve its performance. Stay tuned for upcoming
updates and new features!

## Key Features & Highlights

Tunix is still under development, here's a glimpse of the current features:

- **Supervised Fine-Tuning:**
  - Full Weights Fine-Tuning
  - Parameter-Efficient Fine-Tuning (PEFT) with LoRA/Q-LoRA Layers
- **Reinforcement Learning (RL):**
  - Proximal Policy Optimization (PPO)
  - Group Relative Policy Optimization (GRPO)
  - Token-level Group Sequence Policy Optimization (GSPO-token)
- **Preference Fine-Tuning:**
  - Preference alignments with Direct Preference Optimization (DPO)
- **Knowledge Distillation:**
  - Logit Strategy: A classic approach where the student learns to match the
    teacher's output probability distribution.
  - Attention Transfer & Projection Strategies: Methods to align the attention
    mechanisms between the student and teacher models.
  - Feature Pooling & Projection Strategies: General techniques for matching
    intermediate feature representations, even between models of different
    architectures.
- **Modularity:**
  - Components are designed to be reusable and composable
  - Easy to customize and extend
- **Efficiency:**
  - Native support of common model sharding strategies such as DP, FSDP and TP
  - Designed for distributed training on accelerators (TPU)

## Upcoming

- **Agentic RL Training:**
  - Async Rollout
  - Multi-turn & multi-step support
  - Tool usage
- **Advanced Algorithms:**
  - Addtional state-of-the-art RL and distillation algorithms
- **Scalability:**
  - Multi-host distributed training
  - Optimized rollout with vLLM
- **User Guides:**
  - More advanced RL recipe

## Installation

You can install Tunix in several ways:

1. From PyPI (recommended):

```sh
pip install "google-tunix[prod]"
```

2. Directly from GitHub (latest main branch)

```sh
pip install git+https://github.com/google/tunix
```

3. From source (editable install) If you plan to modify the codebase and run it
   in development mode. If you'd like to install vllm, the tpu-inference
   supported version is not released yet, please follow the instructions to
   install manually
   (https://docs.vllm.ai/en/latest/getting_started/installation/google_tpu.html)
   or download the docker image (vllm/vllm-tpu:v0.11.1) then
   `pip install tpu-inference` for TPU backend:

```sh
git clone https://github.com/google/tunix.git
cd tunix
pip install -e ".[dev]"

# Then install vLLM and tpu-inference
```

## Getting Started

To get started, we have a bunch of detailed examples and tutorials.

- [PEFT Gemma with QLoRA](https://github.com/google/tunix/blob/main/examples/qlora_demo.ipynb)
- [Training Gemma on grade school Math problems using GRPO](https://github.com/google/tunix/blob/main/examples/grpo_demo.ipynb)
- [Logit Distillation using Gemma models](https://github.com/google/tunix/blob/main/examples/logit_distillation.ipynb)

To setup Jupyter notebook on single host GCP TPU VM, please refer to the
[setup script](https://github.com/google/tunix/blob/main/scripts/setup_notebook_tpu_single_host.sh).

We plan to provide clear, concise documentation and more examples in the near
future.

## Contributing and Feedbacks

We welcome contributions! As Tunix is in early development, the contribution
process is still being formalized. A rough draft of the contribution process is
present [here](https://github.com/google/tunix/blob/main/CONTRIBUTING.md). In
the meantime, you can make feature requests, report issues and ask questions in
our
[Tunix GitHub discussion forum](https://github.com/google/tunix/discussions).

## Collaborations and Partnership

[GRL](https://github.com/lmgame-org/GRL/blob/tunix_integration_dev/README.md)
(Game Reinforcement Learning), developed by
[Hao AI Lab](https://hao-ai-lab.github.io/) from UCSD, is an open-source
framework for post-training large language models through multi-turn RL on
challenging games. In collaboration with Tunix, GRL integrates seamless TPU
supportâ€”letting users quickly run scalable, reproducible RL experiments (like
PPO rollouts on Qwen2.5-0.5B-Instruct) on TPU v4 meshes with
[minimal setup](https://github.com/lmgame-org/GRL/blob/tunix_integration_dev/README.md#5-launch-the-quick-test-defaults-to-qwen2505b-supports-4-tpu-v4-with-mesh-22).
This partnership empowers the community to push LLM capabilities further,
combining Tunixâ€™s optimized TPU runtime with GRLâ€™s flexible game RL pipeline for
cutting-edge research and easy reproducibility.

## Stay Tuned!

Thank you for your interest in Tunix. We're working hard to bring you a powerful
and efficient library for LLM post-training. Please follow our progress and
check back for updates!

## Citing Tunix

```bibtex
@misc{tunix2025,
  title={Tunix},
  author={Bao, Tianshu and Wang, Lance and Sharma, Abheesht and Shin, Jiwon and
  Yan, Ann and Tan, Sizhi and Gao, Haoyu and Ha, Jen and Chai, Lin and
  Liu, Dangyi and Iyer, Rakesh and Sahu, Mridul and others},
  year={2025},
  howpublished={\url{https://github.com/google/tunix}},
}
```

## Acknowledgements

Thank you to all our wonderful contributors!

[![Contributors](https://contrib.rocks/image?repo=google/tunix)](https://github.com/google/tunix/graphs/contributors)

# CAL-GRPO: Fine-Grained Credit Assignment for RL Training

âœ… **Integration Tests Passed** - Ready for production experiments!

---

## ðŸŽ¯ Quick Start

### Run Both Experiments (Recommended)

```bash
source .venv/bin/activate

# Small test (10 minutes)
./run_experiments.sh 20 2

# Medium scale (2-3 hours)
./run_experiments.sh 100 4

# Publication quality (6-12 hours)
./run_experiments.sh 500 4
```

### Run Individual Experiments

```bash
# Baseline GRPO
python train_cal.py --num-samples 100 --num-generations 4

# CAL-GRPO (your research)
python train_cal.py --use-cal --num-samples 100 --num-generations 4
```

### Compare Results

```bash
python compare_results.py
```

---

## ðŸ“Š What You Get

After running experiments, you'll see:

```
========================================
BASELINE vs CAL-GRPO COMPARISON
========================================

Test Accuracy (%)     38.50      42.30      +3.80 (+9.9%)
Correct / Total       38/100     42/100             

âœ… CAL improves accuracy by 3.80 percentage points!
========================================
```

---

## ðŸŽ“ Experiment Scale Guide

| Goal | Samples | Time | Command |
|------|---------|------|---------|
| **Quick test** | 20 | 10 min | `./run_experiments.sh 20 2` |
| **Proof of concept** | 100 | 2 hrs | `./run_experiments.sh 100 4` |
| **Workshop paper** | 500 | 6 hrs | `./run_experiments.sh 500 4` |
| **Conference paper** | 7,473 | 24 hrs | `python train_cal.py --use-cal --num-samples -1` |

---

## ðŸ“ Key Files

- **`train_cal.py`** - Main training script (baseline & CAL)
- **`eval_math.py`** - Evaluation system
- **`compare_results.py`** - Results comparison
- **`run_experiments.sh`** - Automated experiment runner
- **`tunix/rl/cal/`** - CAL implementation

---

## ðŸ”§ Configuration

### Use GPT-4 Instead of GPT-3.5

```bash
python train_cal.py --use-cal --cal-model gpt-4
```

### Use Gemini API

Add to `.env`:
```bash
GEMINI_API_KEY=your_key
```

Then run:
```bash
python train_cal.py --use-cal --api-provider gemini --cal-model gemini-1.5-pro-latest
```

### Adjust CAL Parameters

```bash
python train_cal.py --use-cal \
  --negative-reward -2.0 \
  --max-error-span 128 \
  --cal-model gpt-4
```

---

## ðŸ“ˆ View Training Progress

```bash
tensorboard --logdir /tmp/cal_experiments/
# Open browser to http://localhost:6006
```

---

## ðŸ§ª Integration Test Results

âœ… **All systems verified** - See `INTEGRATION_TEST_RESULTS.md`

- Training: âœ… Working
- Evaluation: âœ… Working
- CAL Oracle: âœ… Working
- Comparison: âœ… Working

---

## ðŸ“š Documentation

- **Quick Start**: This file
- **Integration Tests**: `INTEGRATION_TEST_RESULTS.md`
- **Migration Plan**: `cal-to-tunix-migration.plan.md`
- **Demos**: `demos_and_tests/`

---

## ðŸŽ¯ Typical Workflow

1. **Test the setup**
   ```bash
   ./run_experiments.sh 20 2
   python compare_results.py
   ```

2. **Run proof-of-concept**
   ```bash
   ./run_experiments.sh 100 4
   ```

3. **Check if CAL helps**
   - If accuracy improves: Scale up!
   - If not: Tune hyperparameters

4. **Scale to publication quality**
   ```bash
   ./run_experiments.sh 500 4
   ```

5. **Analyze and write paper** ðŸ“

---

## ðŸ” Troubleshooting

### "OPENAI_API_KEY not found"
```bash
echo "OPENAI_API_KEY=sk-..." >> .env
```

### "TPU already in use"
```bash
pkill -9 python
sleep 5
```

### Check TPU availability
```bash
python -c "import jax; print(jax.devices())"
```

---

## âœ¨ What Was Migrated

Your FGC-PPO research from PyTorch/OAT to JAX/Tunix:

- âœ… CAL Oracle (OpenAI + Gemini support)
- âœ… Token-level error mapping
- âœ… Sparse reward construction
- âœ… Fine-grained advantage calculation
- âœ… Full evaluation system
- âœ… Comparison tools

**Everything works and is tested!**

---

## ðŸš€ Ready to Run!

Your CAL research is fully migrated and tested. Run your experiments and publish! ðŸ“„
